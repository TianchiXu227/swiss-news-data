#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‘å£«æ–°é—»GitHub Pageså‘å¸ƒå™¨ - ç®€åŒ–ç‰ˆ
"""

import json
import os
import requests
import time
from datetime import datetime, timezone
from bs4 import BeautifulSoup

class SwissNewsPublisher:
    def __init__(self):
        self.data_dir = 'data'
        self.details_dir = 'details'
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.details_dir, exist_ok=True)
        
    def crawl_news(self):
        """æŠ“å–ç‘å£«æ–°é—»"""
        print("ğŸš€ å¼€å§‹æŠ“å–ç‘å£«æ–°é—»...")
        
        sources = [
            {
                'name': 'ç‘å£«è”é‚¦æ”¿åºœ',
                'url': 'https://www.admin.ch/gov/en/start/documentation/media-releases.html',
                'base_url': 'https://www.admin.ch'
            },
            {
                'name': 'ç‘å£«å¤–äº¤éƒ¨',
                'url': 'https://www.eda.admin.ch/eda/en/fdfa/fdfa/aktuell/news.html',
                'base_url': 'https://www.eda.admin.ch'
            }
        ]
        
        all_news = []
        
        for source in sources:
            try:
                news_items = self._crawl_single_source(source)
                all_news.extend(news_items)
                print(f"âœ… ä» {source['name']} è·å–äº† {len(news_items)} æ¡æ–°é—»")
                time.sleep(2)
            except Exception as e:
                print(f"âŒ æŠ“å– {source['name']} å¤±è´¥: {e}")
        
        # æŒ‰æ—¶é—´æ’åº
        all_news.sort(key=lambda x: x.get('publishTime', ''), reverse=True)
        
        print(f"ğŸ¯ æ€»å…±æŠ“å–äº† {len(all_news)} æ¡æ–°é—»")
        return all_news
    
    def _crawl_single_source(self, source):
        """æŠ“å–å•ä¸ªæ–°é—»æº"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        response = requests.get(source['url'], headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        news_items = []
        
        # æŸ¥æ‰¾æ–°é—»é“¾æ¥
        links = soup.find_all('a', href=True)[:20]
        
        for i, link in enumerate(links):
            try:
                title = link.get_text(strip=True)
                url = link.get('href', '')
                
                if not title or len(title) < 10:
                    continue
                    
                if url and not url.startswith('http'):
                    url = f"{source['base_url']}{url}"
                
                news_item = {
                    'id': f"{source['name']}_{i}_{int(time.time())}",
                    'title': title,
                    'titleChinese': title,
                    'source': source['name'],
                    'url': url,
                    'publishTime': datetime.now(timezone.utc).isoformat(),
                    'category': 'government',
                    'language': 'en',
                    'extractedAt': datetime.now(timezone.utc).isoformat(),
                    'extractedBy': 'github-pages-crawler'
                }
                
                news_items.append(news_item)
                
                if len(news_items) >= 10:  # æ¯ä¸ªæºæœ€å¤š10æ¡
                    break
                    
            except Exception as e:
                continue
        
        return news_items
    
    def publish_to_github(self, news_data):
        """å‘å¸ƒæ–°é—»æ•°æ®åˆ°GitHub Pages"""
        try:
            # ä¿å­˜å®Œæ•´æ–°é—»åˆ—è¡¨
            news_file = os.path.join(self.data_dir, 'news.json')
            with open(news_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'lastUpdated': datetime.now(timezone.utc).isoformat(),
                    'totalCount': len(news_data),
                    'version': '1.0.0',
                    'source': 'GitHub Pages Crawler',
                    'news': news_data
                }, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æœ€æ–°æ–°é—»åˆ—è¡¨
            latest_file = os.path.join(self.data_dir, 'latest.json')
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'lastUpdated': datetime.now(timezone.utc).isoformat(),
                    'count': min(10, len(news_data)),
                    'news': news_data[:10]
                }, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æ–°é—»è¯¦æƒ…
            for news in news_data:
                detail_file = os.path.join(self.details_dir, f"{news['id']}.json")
                with open(detail_file, 'w', encoding='utf-8') as f:
                    json.dump(news, f, ensure_ascii=False, indent=2)
            
            print("âœ… æ‰€æœ‰æ•°æ®æ–‡ä»¶å·²ç”Ÿæˆå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ å‘å¸ƒæ•°æ®å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ ç‘å£«æ–°é—» GitHub Pages å‘å¸ƒå™¨å¯åŠ¨")
    print("=" * 50)
    
    publisher = SwissNewsPublisher()
    
    # æŠ“å–æ–°é—»
    news_data = publisher.crawl_news()
    
    if not news_data:
        print("âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
        return False
    
    # å‘å¸ƒåˆ°GitHub Pages
    success = publisher.publish_to_github(news_data)
    
    if success:
        print("ğŸ‰ æ–°é—»æ•°æ®å·²æˆåŠŸå‘å¸ƒåˆ°GitHub Pages!")
        return True
    else:
        print("âŒ å‘å¸ƒå¤±è´¥")
        return False

if __name__ == '__main__':
    main()
